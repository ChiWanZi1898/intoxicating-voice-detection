{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "8QRBsZLa62xs",
    "outputId": "c7c5cf33-9920-4383-c098-5ab4b31ef3b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a2ae67b685c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALCDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import librosa\n",
    "from dataset import ALCDataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYW42AYn62xw"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4S5Jsdj62xy"
   },
   "outputs": [],
   "source": [
    "SR = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Vg9aioK62x0"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "kfYIuln-62x1",
    "outputId": "f672657b-2c94-4313-8de1-c904de58eafa"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ad4a955455e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malc_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALCDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'E:\\handout'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malc_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_dev1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_dev1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malc_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'd1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_dev2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_dev2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malc_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'd2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malc_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load_meta_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__process_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/dataset.py\u001b[0m in \u001b[0;36m__load_meta_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mdoc_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDOC_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alc_dataset = ALCDataset('E:\\handout')\n",
    "data_train, label_train = alc_dataset.load_data('train', percentage=1.0, num_threads=4)\n",
    "data_dev1, label_dev1 = alc_dataset.load_data('d1', percentage=1.0, num_threads=4)\n",
    "data_dev2, label_dev2 = alc_dataset.load_data('d2', percentage=1.0, num_threads=4)\n",
    "data_test, label_test = alc_dataset.load_data('test', percentage=1.0, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XWENwqi62x3",
    "outputId": "7c54f08c-82f5-4aab-d771-fff6ac22ebc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train: 5400\n",
      "#dev1: 3960\n",
      "#dev2: 1500\n",
      "#test: 3000\n"
     ]
    }
   ],
   "source": [
    "assert len(data_train) == len(label_train)\n",
    "assert len(data_dev1) == len(label_dev1)\n",
    "assert len(data_dev2) == len(label_dev2)\n",
    "assert len(data_test) == len(label_test)\n",
    "\n",
    "print('#train: {}'.format(len(data_train)))\n",
    "print('#dev1: {}'.format(len(data_dev1)))\n",
    "print('#dev2: {}'.format(len(data_dev2)))\n",
    "print('#test: {}'.format(len(data_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5nFUkx-62x5"
   },
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqMgvVKt62x6"
   },
   "outputs": [],
   "source": [
    "class ALCFeature:\n",
    "    def __init__(self, sr):\n",
    "        self.sr = sr\n",
    "        \n",
    "    def delete_silence(self, audio, top_db=20):\n",
    "        result = []\n",
    "        intervals = librosa.effects.split(audio, top_db=top_db, frame_length=2048, hop_length=512)\n",
    "        for interval in intervals:\n",
    "            result.append(audio[interval[0]: interval[1]])\n",
    "        result = np.concatenate(result)\n",
    "        return result\n",
    "        \n",
    "    def get_mfcc(self, data, label, n_mfcc=20, wsize=0.1, concat=5, scale=True):\n",
    "        x_mfcc = []\n",
    "        y_mfcc = []\n",
    "        record_mfcc = []\n",
    "        wsize = int(wsize * self.sr)\n",
    "        for i in tqdm(range(len(data)), ncols=100, ascii=True, desc='MFCC feature'):\n",
    "            audio = self.delete_silence(data[i])\n",
    "            x = librosa.feature.mfcc(audio, sr=self.sr, n_mfcc=n_mfcc, n_fft=2048, hop_length=512, win_length=wsize, window='hann')\n",
    "            if scale:\n",
    "                x = x - np.min(x, axis=1, keepdims=True)\n",
    "            for j in range(x.shape[1] // concat):\n",
    "                slice_ = x[:, j * concat: (j + 1) * concat]\n",
    "                x_mfcc.append(slice_.flatten())\n",
    "                y_mfcc.append(label[i])\n",
    "                record_mfcc.append(i)\n",
    "        x_mfcc = np.stack(x_mfcc)\n",
    "        y_mfcc = np.array(y_mfcc)\n",
    "        record_mfcc = np.array(record_mfcc)\n",
    "        return x_mfcc, y_mfcc, record_mfcc\n",
    "    \n",
    "    def get_pncc(self, data, label):\n",
    "        pass\n",
    "    \n",
    "    def get_cqt(self, data, label, n_chroma=12, wsize=0.1, concat=5, scale=True):\n",
    "        x_cqt = []\n",
    "        y_cqt = []\n",
    "        record_cqt = []\n",
    "        wind = np.hamming(int(wsize * self.sr))\n",
    "        for i in tqdm(range(len(data)), ncols=100, ascii=True, desc='CQT feature'):\n",
    "            audio = self.delete_silence(data[i])\n",
    "            x = librosa.feature.chroma_cqt(audio, sr=self.sr, n_chroma=n_chroma, hop_length=512, window=wind)\n",
    "            if scale:\n",
    "                x = x - np.min(x, axis=1, keepdims=True)\n",
    "            for j in range(x.shape[1] // concat):\n",
    "                slice_ = x[:, j * concat: (j + 1) * concat]\n",
    "                x_cqt.append(slice_.flatten())\n",
    "                y_cqt.append(label[i])\n",
    "                record_cqt.append(i)\n",
    "        x_cqt = np.stack(x_cqt)\n",
    "        y_cqt = np.array(y_cqt)\n",
    "        record_cqt = np.array(record_cqt)\n",
    "        return x_cqt, y_cqt, record_cqt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCYtkcrx62x8"
   },
   "outputs": [],
   "source": [
    "alc_feature = ALCFeature(SR)\n",
    "smote = SMOTE(random_state=0)\n",
    "\n",
    "# # MFCC\n",
    "# x_train, y_train, record_train = alc_feature.get_mfcc(data_train, label_train)\n",
    "# x_balance, y_balance = smote.fit_resample(x_train, y_train)\n",
    "# x_dev1, y_dev1, record_dev1 = alc_feature.get_mfcc(data_dev1, label_dev1)\n",
    "# x_dev2, y_dev2, record_dev2 = alc_feature.get_mfcc(data_dev2, label_dev2)\n",
    "# x_test, y_test, record_test = alc_feature.get_mfcc(data_test, label_test)\n",
    "\n",
    "# CQT test case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivnJYjhR62x-"
   },
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA(n_components=50)\n",
    "pca.fit(x_train)\n",
    "x_train = pca.transform(x_train)\n",
    "x_balance = pca.transform(x_balance)\n",
    "x_dev1 = pca.transform(x_dev1)\n",
    "x_dev2 = pca.transform(x_dev2)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwP14CLS62yA"
   },
   "source": [
    "### Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "maXneWMA62yA"
   },
   "outputs": [],
   "source": [
    "class ALCModel:\n",
    "    def __init__(self, method, verbose=None):\n",
    "        if method == 'lr':\n",
    "            if verbose is None:\n",
    "                verbose = 0\n",
    "            self.clf = LogisticRegression(verbose=verbose)\n",
    "        elif method == 'svm':\n",
    "            if verbose is None:\n",
    "                verbose = False\n",
    "            self.clf = SVC(C=1.0, kernel='rbf', verbose=verbose)\n",
    "        elif method == 'forest':\n",
    "            if verbose is None:\n",
    "                verbose = 0\n",
    "            self.clf = RandomForestClassifier(n_estimators=100, verbose=verbose)\n",
    "        elif method == 'adaboost':\n",
    "            self.clf = AdaBoostClassifier(n_estimators=100)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.clf.fit(x, y)\n",
    "    \n",
    "    def predict(self, x, record):\n",
    "        prediction = []\n",
    "        probability = []\n",
    "        raw_pred = self.clf.predict(x)\n",
    "        for i in range(record[-1] + 1):\n",
    "            this_pred = raw_pred[record == i]\n",
    "            if len(this_pred) == 0:\n",
    "                this_prob = np.random.uniform(low=0.0, high=1.0)\n",
    "                this_pred = np.random.choice([0, 1])              \n",
    "            else:\n",
    "                this_prob = np.mean(this_pred)\n",
    "                this_pred = np.argmax(np.bincount(this_pred))\n",
    "            prediction.append(this_pred)\n",
    "            probability.append(this_prob)\n",
    "        prediction = np.array(prediction)\n",
    "        probability = np.array(probability)\n",
    "        return prediction, probability\n",
    "    \n",
    "    def evaluate(self, x, record, label, roc=False):\n",
    "        pred, prob = self.predict(x, record)\n",
    "        acc = np.mean(pred == label)\n",
    "        report = sklearn.metrics.classification_report(label, pred)\n",
    "        if roc:\n",
    "            fpr, tpr, thresholds = sklearn.metrics.roc_curve(label, prob)\n",
    "            plt.figure()\n",
    "            plt.plot(fpr, tpr)\n",
    "            plt.title('ROC Curve')\n",
    "            plt.xlabel('False positive rate')\n",
    "            plt.ylabel('True positive rate')\n",
    "            plt.show()\n",
    "        return acc, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbY9ckig62yC"
   },
   "outputs": [],
   "source": [
    "wsize_list =  [0.1,0.2,0.3,0.4]\n",
    "n_list = [50,60,70,80,90,100]\n",
    "\n",
    "\n",
    "acc_list_svm = []\n",
    "for w in (wsize_list):\n",
    "  for n in (n_list):\n",
    "    x_train, y_train, record_train = alc_feature.get_cqt(data_train, label_train, wsize = w)\n",
    "    x_balance, y_balance = smote.fit_resample(x_train, y_train)\n",
    "    x_dev1, y_dev1, record_dev1 = alc_feature.get_cqt(data_dev1, label_dev1, wsize = w)\n",
    "    x_dev2, y_dev2, record_dev2 = alc_feature.get_cqt(data_dev2, label_dev2, wsize = w)\n",
    "    x_test, y_test, record_test = alc_feature.get_cqt(data_test, label_test, wsize = w)\n",
    "\n",
    "    pca = sklearn.decomposition.PCA(n_components=n)\n",
    "    pca.fit(x_train)\n",
    "    x_train = pca.transform(x_train)\n",
    "    x_balance = pca.transform(x_balance)\n",
    "    x_dev1 = pca.transform(x_dev1)\n",
    "    x_dev2 = pca.transform(x_dev2)\n",
    "    x_test = pca.transform(x_test)\n",
    "\n",
    "    model = ALCModel('svm')\n",
    "    # model.fit(x_train, y_train)\n",
    "    model.fit(x_balance, y_balance)\n",
    "    acc, report = model.evaluate(x_test, record_test, label_test, roc=True)\n",
    "    acc_list_svm.append(acc)\n",
    "    print(report)\n",
    "\n",
    "acc_list_lr = []\n",
    "for w in (wsize_list):\n",
    "  for n in (n_list):\n",
    "    x_train, y_train, record_train = alc_feature.get_cqt(data_train, label_train, wsize = w)\n",
    "    x_balance, y_balance = smote.fit_resample(x_train, y_train)\n",
    "    x_dev1, y_dev1, record_dev1 = alc_feature.get_cqt(data_dev1, label_dev1, wsize = w)\n",
    "    x_dev2, y_dev2, record_dev2 = alc_feature.get_cqt(data_dev2, label_dev2, wsize = w)\n",
    "    x_test, y_test, record_test = alc_feature.get_cqt(data_test, label_test, wsize = w)\n",
    "\n",
    "    pca = sklearn.decomposition.PCA(n_components=n)\n",
    "    pca.fit(x_train)\n",
    "    x_train = pca.transform(x_train)\n",
    "    x_balance = pca.transform(x_balance)\n",
    "    x_dev1 = pca.transform(x_dev1)\n",
    "    x_dev2 = pca.transform(x_dev2)\n",
    "    x_test = pca.transform(x_test)\n",
    "\n",
    "    model = ALCModel('lr')\n",
    "    # model.fit(x_train, y_train)\n",
    "    model.fit(x_balance, y_balance)\n",
    "    acc, report = model.evaluate(x_test, record_test, label_test, roc=True)\n",
    "    acc_list_lr.append(acc)\n",
    "    print(report)\n",
    "\n",
    "acc_list_forest = []\n",
    "for w in (wsize_list):\n",
    "  for n in (n_list):\n",
    "    x_train, y_train, record_train = alc_feature.get_cqt(data_train, label_train, wsize = w)\n",
    "    x_balance, y_balance = smote.fit_resample(x_train, y_train)\n",
    "    x_dev1, y_dev1, record_dev1 = alc_feature.get_cqt(data_dev1, label_dev1, wsize = w)\n",
    "    x_dev2, y_dev2, record_dev2 = alc_feature.get_cqt(data_dev2, label_dev2, wsize = w)\n",
    "    x_test, y_test, record_test = alc_feature.get_cqt(data_test, label_test, wsize = w)\n",
    "\n",
    "\n",
    "    pca = sklearn.decomposition.PCA(n_components=n)\n",
    "    pca.fit(x_train)\n",
    "    x_train = pca.transform(x_train)\n",
    "    x_balance = pca.transform(x_balance)\n",
    "    x_dev1 = pca.transform(x_dev1)\n",
    "    x_dev2 = pca.transform(x_dev2)\n",
    "    x_test = pca.transform(x_test)\n",
    "\n",
    "    model = ALCModel('forest')\n",
    "    # model.fit(x_train, y_train)\n",
    "    model.fit(x_balance, y_balance)\n",
    "    acc, report = model.evaluate(x_test, record_test, label_test, roc=True)\n",
    "    acc_list_forest.append(acc)\n",
    "    print(report)\n",
    "\n",
    "acc_list_ada = []\n",
    "for w in (wsize_list):\n",
    "  for n in (n_list):\n",
    "    x_train, y_train, record_train = alc_feature.get_cqt(data_train, label_train, wsize = w)\n",
    "    x_balance, y_balance = smote.fit_resample(x_train, y_train)\n",
    "    x_dev1, y_dev1, record_dev1 = alc_feature.get_cqt(data_dev1, label_dev1, wsize = w)\n",
    "    x_dev2, y_dev2, record_dev2 = alc_feature.get_cqt(data_dev2, label_dev2, wsize = w)\n",
    "    x_test, y_test, record_test = alc_feature.get_cqt(data_test, label_test, wsize = w)\n",
    "\n",
    "\n",
    "    pca = sklearn.decomposition.PCA(n_components=n)\n",
    "    pca.fit(x_train)\n",
    "    x_train = pca.transform(x_train)\n",
    "    x_balance = pca.transform(x_balance)\n",
    "    x_dev1 = pca.transform(x_dev1)\n",
    "    x_dev2 = pca.transform(x_dev2)\n",
    "    x_test = pca.transform(x_test)\n",
    "\n",
    "    model = ALCModel('adaboost')\n",
    "    # model.fit(x_train, y_train)\n",
    "    model.fit(x_balance, y_balance)\n",
    "    acc, report = model.evaluate(x_test, record_test, label_test, roc=True)\n",
    "    print(report)\n",
    "    acc_list_ada.append(acc)\n",
    "    print(report)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhvJlOg662yE"
   },
   "outputs": [],
   "source": [
    "model = ALCModel('lr')\n",
    "# model.fit(x_train, y_train)\n",
    "model.fit(x_balance, y_balance)\n",
    "acc, report = model.evaluate(x_test, record_test, label_test, roc=True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XUrOI-v62yG"
   },
   "outputs": [],
   "source": [
    "model = ALCModel('forest')\n",
    "# model.fit(x_train, y_train)\n",
    "model.fit(x_balance, y_balance)\n",
    "acc, report = model.evaluate(x_test, record_test, label_test, roc=True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUjJww0z62yJ"
   },
   "outputs": [],
   "source": [
    "model = ALCModel('adaboost')\n",
    "# model.fit(x_train, y_train)\n",
    "model.fit(x_balance, y_balance)\n",
    "acc, report = model.evaluate(x_test, record_test, label_test, roc=True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTog0ZVJ62yL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "feature.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
